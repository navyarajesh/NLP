{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "part 1\n",
        "- Use a pretrained word2vec model (example - `word2vec-google-news-300` )\n",
        "- Pick any 5 words of your choice and find the model similar words for each of these 5 words.\n",
        "- Just like the experiment from the lecture where we checked `king - man + woman ~= queen`  - come up with 2-3 similar examples and test them with the pre-trained word2vec model’s vectors.\n"
      ],
      "metadata": {
        "id": "erVGLFUIEl50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install gensim scikit-learn tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nqrJRrhN-Y8",
        "outputId": "2a15c843-74dc-457e-97a0-90fc2b414b6e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load GloVe Twitter embeddings\n",
        "model = api.load(\"glove-twitter-200\")\n",
        "\n",
        "# Task 1: Find similar words\n",
        "words = ['coffee', 'music', 'happy', 'school', 'football']\n",
        "\n",
        "print(\"Task 1: Similar Words\\n\")\n",
        "for word in words:\n",
        "    print(f\"Top 5 similar words to '{word}':\")\n",
        "    try:\n",
        "        for similar_word, score in model.most_similar(word, topn=5):\n",
        "            print(f\"  {similar_word} (similarity: {score:.4f})\")\n",
        "    except KeyError:\n",
        "        print(f\"  '{word}' not found in vocabulary.\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43r5B07pNY9e",
        "outputId": "cff6f872-39fd-437e-e562-ee2a45dbfadd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 758.5/758.5MB downloaded\n",
            "Task 1: Similar Words\n",
            "\n",
            "Top 5 similar words to 'coffee':\n",
            "  tea (similarity: 0.7670)\n",
            "  starbucks (similarity: 0.7393)\n",
            "  coffe (similarity: 0.7360)\n",
            "  beer (similarity: 0.7237)\n",
            "  drink (similarity: 0.7041)\n",
            "\n",
            "Top 5 similar words to 'music':\n",
            "  songs (similarity: 0.7505)\n",
            "  song (similarity: 0.7472)\n",
            "  listen (similarity: 0.7371)\n",
            "  listening (similarity: 0.7023)\n",
            "  radio (similarity: 0.6813)\n",
            "\n",
            "Top 5 similar words to 'happy':\n",
            "  birthday (similarity: 0.8999)\n",
            "  day (similarity: 0.8071)\n",
            "  bday (similarity: 0.7735)\n",
            "  wish (similarity: 0.7572)\n",
            "  merry (similarity: 0.7265)\n",
            "\n",
            "Top 5 similar words to 'school':\n",
            "  college (similarity: 0.7961)\n",
            "  class (similarity: 0.7617)\n",
            "  tomorrow (similarity: 0.7286)\n",
            "  high (similarity: 0.7116)\n",
            "  kids (similarity: 0.7063)\n",
            "\n",
            "Top 5 similar words to 'football':\n",
            "  soccer (similarity: 0.8489)\n",
            "  basketball (similarity: 0.7916)\n",
            "  sports (similarity: 0.7718)\n",
            "  players (similarity: 0.7588)\n",
            "  baseball (similarity: 0.7572)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load GloVe Twitter embeddings\n",
        "model = api.load(\"glove-twitter-200\")\n",
        "\n",
        "# Define analogies\n",
        "analogy_examples = [\n",
        "    (\"doctor\", \"hospital\", \"school\"),     # → teacher\n",
        "    (\"basketball\", \"court\", \"ring\"),      # → boxing\n",
        "    (\"iphone\", \"apple\", \"samsung\"),       # → galaxy\n",
        "    (\"batman\", \"gotham\", \"krypton\"),      # → superman\n",
        "]\n",
        "\n",
        "print(\" Better Analogy Test Results:\\n\")\n",
        "\n",
        "for a, b, c in analogy_examples:\n",
        "    try:\n",
        "        result = model.most_similar(positive=[a, c], negative=[b], topn=1)\n",
        "        print(f\"{a} - {b} + {c} = {result[0][0]} (similarity: {result[0][1]:.4f})\")\n",
        "    except KeyError as e:\n",
        "        print(f\" One of the words is missing in vocabulary: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBVTm0IrPl-w",
        "outputId": "9c4ff217-9c09-4d93-ab02-80c72d082d00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Better Analogy Test Results:\n",
            "\n",
            "doctor - hospital + school = teacher (similarity: 0.6118)\n",
            "basketball - court + ring = rings (similarity: 0.6044)\n",
            "iphone - apple + samsung = galaxy (similarity: 0.7864)\n",
            "batman - gotham + krypton = superman (similarity: 0.4571)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2\n",
        "\n",
        "Build a movie review sentiment classifier using `WordVectors`\n",
        "\n",
        "- Dataset: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/data\n",
        "- Tasks:\n",
        "    1. Perform text EDA\n",
        "    2. Clean the text - remove noisy tokens like punctuations and stopwords\n",
        "    3. Train an ML model of your choice using:\n",
        "        1. A pre-trained W2V model’s vector (pick any model from the web)\n",
        "        2. Custom Skip-gram vectors\n",
        "        3. Custom CBoW vectors\n",
        "        4. Custom FastText vectors\n",
        "    4. Tabulate the model performance stats."
      ],
      "metadata": {
        "id": "Wm8PglNYNhgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# 2. Load Data\n",
        "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDQo79NXHXxd",
        "outputId": "3f2cab78-ed32-4bfa-fdcc-63fb44aeb3ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Clean Text\n",
        "def clean_text(text):\n",
        "    text = re.sub(\"<.*?>\", \" \", text)  # HTML\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)  # Non-letter chars\n",
        "    tokens = text.lower().split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "df[\"cleaned_tokens\"] = df[\"review\"].progress_apply(clean_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_3EtsbnH8ju",
        "outputId": "57af6a7e-06c2-4f1f-c019-b1ca574b4ca4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:04<00:00, 11516.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Helper to convert tokens to average vector\n",
        "def get_avg_vector(tokens, model, dim):\n",
        "    vec = np.zeros(dim)\n",
        "    count = 0\n",
        "    for word in tokens:\n",
        "        if word in model:\n",
        "            vec += model[word]\n",
        "            count += 1\n",
        "    return vec / count if count > 0 else vec\n",
        "\n",
        "def build_features(df, model, dim):\n",
        "    return np.vstack(df[\"cleaned_tokens\"].progress_apply(lambda x: get_avg_vector(x, model, dim)))\n"
      ],
      "metadata": {
        "id": "tsNlSOcOID3P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train + Evaluate Classifier\n",
        "def train_and_evaluate(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train, y_train)\n",
        "    return accuracy_score(y_test, clf.predict(X_test))\n"
      ],
      "metadata": {
        "id": "JRYCU6kbITuD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from gensim.downloader import load\n",
        "\n",
        "print(\"Loading GloVe pretrained vectors...\")\n",
        "w2v_pretrained = load(\"glove-wiki-gigaword-100\")  # 100-dimensional GloVe\n",
        "X_w2v = build_features(df, w2v_pretrained, 100)\n",
        "acc_w2v = train_and_evaluate(X_w2v, df['label'])\n",
        "print(\"Pretrained GloVe Accuracy:\", acc_w2v)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PirEzsmmIWtf",
        "outputId": "5f254322-3de4-44f7-fa9f-705575ecbc42"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe pretrained vectors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:12<00:00, 3943.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained GloVe Accuracy: 0.7983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Custom Skip-Gram\n",
        "print(\"Training custom Skip-Gram model...\")\n",
        "w2v_sg = Word2Vec(sentences=df[\"cleaned_tokens\"], vector_size=100, window=5, min_count=5, sg=1, workers=4)\n",
        "X_sg = build_features(df, w2v_sg.wv, 100)\n",
        "acc_sg = train_and_evaluate(X_sg, df['label'])\n",
        "print(\"Custom Skip-Gram accuracy:\", acc_sg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2hPK0-iJPjo",
        "outputId": "93682538-b575-4e1a-fb0e-43b720739524"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training custom Skip-Gram model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:12<00:00, 4016.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Skip-Gram accuracy: 0.8774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Custom CBOW\n",
        "print(\"Training custom CBOW model...\")\n",
        "w2v_cbow = Word2Vec(sentences=df[\"cleaned_tokens\"], vector_size=100, window=5, min_count=5, sg=0, workers=4)\n",
        "X_cbow = build_features(df, w2v_cbow.wv, 100)\n",
        "acc_cbow = train_and_evaluate(X_cbow, df['label'])\n",
        "print(\"Custom CBOW accuracy:\", acc_cbow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVK2NvF3J-oo",
        "outputId": "0fb71501-535d-4931-b98c-9369d8c6caa1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training custom CBOW model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:12<00:00, 3858.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom CBOW accuracy: 0.8657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. FastText\n",
        "print(\"Training FastText model...\")\n",
        "fasttext_model = FastText(sentences=df[\"cleaned_tokens\"], vector_size=100, window=5, min_count=5, workers=4)\n",
        "X_ft = build_features(df, fasttext_model.wv, 100)\n",
        "acc_ft = train_and_evaluate(X_ft, df['label'])\n",
        "print(\"FastText accuracy:\", acc_ft)\n",
        "\n",
        "# 10. Summary Table\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Pretrained Word2Vec\", \"Custom Skip-Gram\", \"Custom CBOW\", \"Custom FastText\"],\n",
        "    \"Accuracy\": [acc_w2v, acc_sg, acc_cbow, acc_ft]\n",
        "})\n",
        "\n",
        "print(\"\\n Performance Summary:\")\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B07ge7W2KAFu",
        "outputId": "68a89214-6ab9-4d3e-c7e5-8f8e03f8a48d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training FastText model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:16<00:00, 2982.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText accuracy: 0.8512\n",
            "\n",
            " Performance Summary:\n",
            "                 Model  Accuracy\n",
            "0  Pretrained Word2Vec    0.7983\n",
            "1     Custom Skip-Gram    0.8774\n",
            "2          Custom CBOW    0.8657\n",
            "3      Custom FastText    0.8512\n"
          ]
        }
      ]
    }
  ]
}